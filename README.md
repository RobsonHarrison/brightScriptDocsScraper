# brightScriptDocsScraper

A Python scraper that recursively downloads Roku/BrightScript documentation from [developer.roku.com](https://developer.roku.com) and stores it as clean markdown files. Runs automatically via GitHub Actions on a weekly schedule.

## ğŸ“ Project Structure

```
brightScriptDocsScraper/
â”œâ”€â”€ .github/workflows/      # GitHub Actions (weekly auto-scrape)
â”œâ”€â”€ scraper/                # Scraper source code
â”‚   â””â”€â”€ WebScraper.py
â”œâ”€â”€ roku_docs/              # ğŸ“š Scraped documentation
â”‚   â”œâ”€â”€ docs/               #    Developer guides & API references
â”‚   â”œâ”€â”€ trc-docs/           #    TRC documentation
â”‚   â””â”€â”€ results.json        #    Scrape results & broken link report
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âœ¨ Features

- **Automated weekly updates** via GitHub Actions
- Deep crawls the Roku developer documentation
- Converts pages to clean markdown files
- Tracks broken links and their source pages
- Handles malformed URLs and retries with corrections
- Duplicate protection (skips already-saved files)

## ğŸš€ Quick Start

### Local Usage

```bash
# Install dependencies
pip install -r requirements.txt
crawl4ai-setup

# Run the scraper
python scraper/WebScraper.py

# Custom options
python scraper/WebScraper.py --output-dir ./roku_docs --max-depth 5 --max-pages 500

# Verbose mode
python scraper/WebScraper.py --verbose
```

### Options

| Option | Default | Description |
|--------|---------|-------------|
| `--output-dir` | `./roku_docs` | Directory to save markdown files |
| `--max-depth` | `10` | Maximum link depth to crawl |
| `--max-pages` | `2000` | Maximum number of pages to crawl |
| `--verbose`, `-v` | `false` | Show detailed crawl4ai logging |

## ğŸ¤– GitHub Actions

The scraper runs automatically every **Sunday at 2:00 AM UTC**. New/updated documentation is committed directly to the repository.

To run manually: **Actions** â†’ **Scrape Roku Documentation** â†’ **Run workflow**

## ğŸ“„ Output

The `roku_docs/` folder contains:
- Markdown files preserving the original URL path structure
- `results.json` with scrape summary:
  - `saved` - Successfully saved pages with source URLs
  - `skipped` - Count of already-existing files
  - `failed` - Failed pages with error reasons (useful for finding broken links on Roku's site)

